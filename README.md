# Audience Attention Estimation (Android Demo)

An on-device Android demo that estimates **audience attention** in real time using
CameraX + ML Kit Face Detection.

The system is designed for **presentation / demo scenarios**, where a presenter wants
to objectively measure audience engagement without invasive tracking.





Perfect â€” here is the clean **raw Markdown version**.
You can copy-paste this directly into `README.md`.

---

```markdown
# ğŸ¤ AI Presentation Feedback System

An end-to-end presentation coaching pipeline that analyzes recorded presentation videos and generates automated feedback using:

- ğŸ“± Mobile attention tracking (face/engagement logs)
- ğŸ¬ Low-attention clip extraction
- ğŸ–¼ Vision-Language Model (Qwen2-VL via LM Studio)
- ğŸ–¥ Gradio demo interface

This system provides post-presentation feedback on:

- Attention trends  
- Low-engagement moments  
- Visual posture / gaze behavior  
- Overall delivery quality  

---

# ğŸ“Œ System Overview

## Workflow

```

Phone App
â†“
presentation_whole.mp4
presentation_self.json
â†“
PC Pipeline
â†“
Low-attention clip extraction
â†“
Frame grid generation
â†“
Vision-Language Model (Qwen2-VL)
â†“
AI feedback

```

---

# ğŸ“ Project Structure

```

.
â”œâ”€â”€ presentation_whole.mp4
â”œâ”€â”€ presentation_self.json
â”œâ”€â”€ extract_low_attention_clips.py
â”œâ”€â”€ presentation_feedback_process.py
â”œâ”€â”€ gradio_app.py
â”œâ”€â”€ clips/
â”œâ”€â”€ out/
â””â”€â”€ README.md

```

---

# ğŸ“± Phone Output Format

After the presentation ends, the phone sends:

## 1ï¸âƒ£ Full Video

```

presentation_whole.mp4

```

## 2ï¸âƒ£ Attention Logs

```

presentation_self.json

````

### JSON Structure

```json
[
  {
    "tsMs": 0,
    "score100_1min": 88,
    "faces": 1,
    "confidence": 0.95
  },
  {
    "tsMs": 30000,
    "score100_1min": 85,
    "faces": 1,
    "confidence": 0.93
  }
]
````

### Fields

| Field           | Description               |
| --------------- | ------------------------- |
| `tsMs`          | Timestamp in milliseconds |
| `score100_1min` | Attention score (0â€“100)   |
| `faces`         | Number of detected faces  |
| `confidence`    | Detection confidence      |

The PC pipeline automatically converts:

* `tsMs â†’ timestamp (seconds)`
* `score100_1min â†’ attention_score`

---

# ğŸ¬ Step 1: Extract Low-Attention Clips

This script:

* Computes overall attention mean
* Finds timestamps below threshold
* Merges nearby timestamps
* Extracts video clips around low-attention moments

### Run:

```bash
python extract_low_attention_clips.py \
  --video presentation_whole.mp4 \
  --scores presentation_self.json
```

### Optional tuning:

```bash
--before 20
--after 20
--threshold-mode below_mean_minus_std
--backend ffmpeg
```

### Output

```
clips/
  clip_000_90.0s_130.0s.mp4
  clip_001_150.0s_190.0s.mp4
```

---

# ğŸ–¼ Step 2: Visual Feedback Generation

`presentation_feedback_process.py`:

1. Extracts representative frames
2. Creates a grid image
3. Sends image + summary to Qwen2-VL
4. Receives structured AI feedback

---

# ğŸ¤– Vision Model Setup (LM Studio)

## Requirements

* LM Studio installed
* OpenAI-compatible server enabled
* Vision model loaded (e.g., `qwen2-vl-2b-instruct`)

Server must run at:

```
http://127.0.0.1:1234
```

Test in browser:

```
http://127.0.0.1:1234/v1/models
```

---

# ğŸ–¥ Step 3: Run Gradio Demo

Launch UI:

```bash
python gradio_app.py
```

Gradio runs at:

```
http://127.0.0.1:7860
```

Upload video â†’ Get AI feedback.

---

# ğŸ§  Feedback Logic

The system combines:

## ğŸ“Š Attention Analysis

* Overall mean attention
* Low-attention regions
* Recovery trends

## ğŸ–¼ Visual Analysis

* Posture
* Gaze direction
* Body movement
* Engagement cues

## ğŸ“ Final Output

Structured coaching feedback:

* Strengths
* Weak moments
* Specific suggestions
* Actionable improvement tips

---

# âš™ Dependencies

Install:

```bash
pip install pandas opencv-python gradio requests
```

If using ffmpeg backend:

```bash
brew install ffmpeg
```

or

```bash
sudo apt install ffmpeg
```

---

# ğŸ§ª Demo Mode

For testing without phone:

1. Generate synthetic attention JSON
2. Use any presentation video
3. Run pipeline

---

# ğŸš€ Future Improvements

* Real-time feedback mode
* Confidence-weighted scoring
* Multi-person audience tracking
* Speech-to-text analysis
* Slide-change detection
* Attention visualization graph in UI

---

# ğŸ— Architecture Diagram

```
Phone App
   â”œâ”€â”€ Video Recording
   â”œâ”€â”€ Face Tracking
   â””â”€â”€ Attention Scoring
         â†“
PC Backend
   â”œâ”€â”€ Low Attention Detection
   â”œâ”€â”€ Clip Extraction
   â”œâ”€â”€ Frame Grid Generation
   â””â”€â”€ Vision-Language Model
         â†“
AI Feedback
         â†“
Gradio UI
```

---

# ğŸ¯ Goal

Provide accessible, AI-powered presentation coaching that:

* Identifies weak engagement moments
* Analyzes non-verbal cues
* Offers actionable improvement advice
* Works entirely offline (local VLM)

```

---

If you later want a cleaner GitHub-style version without emojis (more professional), I can rewrite it in a minimal engineering tone as well.
```
